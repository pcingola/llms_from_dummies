{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Debug Callbacks\n",
    "\n",
    "This is an example on how to create a debug handler to get detailed information about the LangChain, so it's easier to debug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "import requests\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optional: Setting environment variable for tracing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"LANGCHAIN_TRACING\"] = \"true\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read OpenAI Key from `$HOME/.openai`\n",
    "# File format: Only 1 line with the key\n",
    "\n",
    "HOME = os.environ[\"HOME\"]\n",
    "openai_file = f\"{HOME}/.openai\"\n",
    "openai_key = None\n",
    "\n",
    "# Read it from config file\n",
    "with open(openai_file) as f:\n",
    "    openai_key = f.readline().strip()\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = openai_key"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debug Callback\n",
    "\n",
    "Reference: https://python.langchain.com/en/latest/modules/callbacks/getting_started.html"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print prompts in nicer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_lines(text, line_prefix=\"\\t\\t| \"):\n",
    "    for line in text.split(\"\\n\"):\n",
    "        print(line_prefix + line)\n",
    "\n",
    "\n",
    "def print_prompts(prompts):\n",
    "    if len(prompts) == 1:\n",
    "        prompt = prompts[0]\n",
    "        if len(prompt.split(\"\\n\")) == 1:\n",
    "            print(f\"\\tprompt='{prompt}'\")\n",
    "        else:\n",
    "            print(\"\\tprompt=\")\n",
    "            print_lines(prompts[0])\n",
    "    else:\n",
    "        print(f\"\\tprompts={len(prompts)}\")\n",
    "        for i, prompt in enumerate(prompts):\n",
    "            print(f\"\\n\\tprompt[{i}]\")\n",
    "            print_lines(prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print generated output in nicer format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_generations(generations):\n",
    "    print(f\"generations: {generations}\")\n",
    "    if len(generations) == 1:\n",
    "        txt = generations[0].text\n",
    "        if len(txt.split(\"\\n\")) == 1:\n",
    "            print(f\"\\tgeneration='{txt}'\")\n",
    "        else:\n",
    "            print(\"\\tgeneration=\")\n",
    "            print_lines(txt)\n",
    "    else:\n",
    "        print(f\"\\tgenerations={len(generations)}\")\n",
    "        for i, generation in enumerate(generations):\n",
    "            print(f\"\\n\\tgeneration[{i}]\")\n",
    "            print_lines(generation.text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Call Back object for debugging purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Union\n",
    "from langchain.callbacks.base import BaseCallbackHandler\n",
    "from langchain.input import print_text\n",
    "from langchain.schema import AgentAction, AgentFinish, LLMResult\n",
    "\n",
    "\n",
    "class DebugCallbackHandler(BaseCallbackHandler):\n",
    "    \"\"\"Callback Handler that prints to std out.\"\"\"\n",
    "\n",
    "    def __init__(self, color: Optional[str] = None) -> None:\n",
    "        \"\"\"Initialize callback handler.\"\"\"\n",
    "        self.color = color\n",
    "        self.debug = True\n",
    "\n",
    "    def on_llm_start(\n",
    "        self, serialized: Dict[str, Any], prompts: List[str], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Print out the prompts.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_llm_start:\\n\\tserialized={serialized}\")\n",
    "            print_prompts(prompts)\n",
    "\n",
    "    def on_llm_end(self, response: LLMResult, **kwargs: Any) -> None:\n",
    "        \"\"\"Do nothing.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_llm_end:\\n\\tllm_output: {response.llm_output}\")\n",
    "            for gens in response.generations:\n",
    "                print_generations(gens)\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs: Any) -> None:\n",
    "        \"\"\"Do nothing.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_llm_new_token: token={token}\")\n",
    "\n",
    "    def on_llm_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Do nothing.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_chain_start(\n",
    "        self, serialized: Dict[str, Any], inputs: Dict[str, Any], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Print out that we are entering a chain.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_chain_start:\\n\\tserialized: {serialized}\\n\\tinputs: {inputs}\")\n",
    "\n",
    "    def on_chain_end(self, outputs: Dict[str, Any], **kwargs: Any) -> None:\n",
    "        \"\"\"Print out that we finished a chain.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_chain_end:\\n\\toutputs={outputs}\")\n",
    "\n",
    "    def on_chain_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Do nothing.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_tool_start(\n",
    "        self,\n",
    "        serialized: Dict[str, Any],\n",
    "        input_str: str,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Do nothing.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_tool_start:\\n\\tserialized={serialized}\\n\\tinput_str={input_str}\")\n",
    "\n",
    "    def on_agent_action(\n",
    "        self, action: AgentAction, color: Optional[str] = None, **kwargs: Any\n",
    "    ) -> Any:\n",
    "        \"\"\"Run on agent action.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_agent_action:\\n\\taction={action}\")\n",
    "\n",
    "    def on_tool_end(\n",
    "        self,\n",
    "        output: str,\n",
    "        color: Optional[str] = None,\n",
    "        observation_prefix: Optional[str] = None,\n",
    "        llm_prefix: Optional[str] = None,\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"If not the final action, print out observation.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_tool_end:\\n\\toutput={output}\\n\\tobservation_prefix={observation_prefix}\\n\\tllm_prefix={llm_prefix}\")\n",
    "\n",
    "    def on_tool_error(\n",
    "        self, error: Union[Exception, KeyboardInterrupt], **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Do nothing.\"\"\"\n",
    "        pass\n",
    "\n",
    "    def on_text(\n",
    "        self,\n",
    "        text: str,\n",
    "        color: Optional[str] = None,\n",
    "        end: str = \"\",\n",
    "        **kwargs: Any,\n",
    "    ) -> None:\n",
    "        \"\"\"Run when agent ends.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_text:\\n\\ttext={text}\\n\\tend={end}\")\n",
    "\n",
    "    def on_agent_finish(\n",
    "        self, finish: AgentFinish, color: Optional[str] = None, **kwargs: Any\n",
    "    ) -> None:\n",
    "        \"\"\"Run on agent end.\"\"\"\n",
    "        if self.debug:\n",
    "            print(f\"on_agent_finish:\\n\\tfinish={finish}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: How to use Debug on one call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = DebugCallbackHandler()\n",
    "llm = OpenAI(temperature=0.9, callbacks=[handler])\n",
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
