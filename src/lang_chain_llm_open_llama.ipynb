{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: Using our own LLM and API servre with LangChain\n",
    "\n",
    "This is an example on how to create a LLM class for our own Llama server.\n",
    "\n",
    "**Notes:**\n",
    "- This assumes the server is running using the example API (in this same code repository) on a server running on our own EC2 instance.\n",
    "- You can also use SSH / SSM port forwarding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:8000 \"GET /sessions?name=default HTTP/1.1\" 404 22\n",
      "WARNING:langchain_core.tracers.langchain_v1:Failed to load default session, using empty session: 0\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): 127.0.0.1:8000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_llm_start:\n",
      "\tserialized={'lc': 1, 'type': 'not_implemented', 'id': ['langchain_helpers', 'llama_api_server', 'OpenLlamaLLM'], 'repr': \"OpenLlamaLLM(callbacks=[<langchain_helpers.debug_callback_handler.DebugCallbackHandler object at 0x107b4d890>], host='127.0.0.1', url='http://127.0.0.1:8000/v1/completions/')\"}\n",
      "\tprompt='Hello, my name is '\n",
      "OpenLlamaLLM prompt (len:18): 'Hello, my name is '\n",
      "on_llm_new_token: token=Hello, my name is \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:http://127.0.0.1:8000 \"POST /v1/completions/ HTTP/1.1\" 200 1368\n",
      "DEBUG:langchain_helpers.llama_api_server:OpenLlamaLLM response: <Response [200]>\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTP connection (1): localhost:8000\n",
      "DEBUG:urllib3.connectionpool:http://localhost:8000 \"POST /llm-runs HTTP/1.1\" 404 22\n",
      "WARNING:langchain_core.tracers.langchain_v1:Failed to persist run: {\"detail\":\"Not Found\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_llm_end:\n",
      "\tllm_output: None\n",
      "generations: [Generation(text='Hello, my name is William, and I am a recovering alcoholic. I have been clean and sober for ten years, and I am very proud of that fact. I am also proud to say that I have made a lot of changes in my life since my last drink. I am, however, still struggling with the same thing that I was struggling with when I was using: I am lonely.\\nI am very fortunate to live in a small town. I have a lot of friends here. My wife and I have been married for 12 years, and we have two beautiful children. We have good jobs. I have a good job, and I love it.\\nIn spite of all of this, I still feel lonely. My wife works a lot, and I have to spend a lot of time with the kids. I have to get up really early to get them ready for school, and I am often exhausted by the time they come home from school. I have no problem with this. I love them. I just wish that I had some time with them when they were not occupied with schoolwork, homework, etc.\\nI know that this is something that a lot of people go through. I am not the only person who is lonely, even in this small town.')]\n",
      "\tgeneration=\n",
      "\t\t| Hello, my name is William, and I am a recovering alcoholic. I have been clean and sober for ten years, and I am very proud of that fact. I am also proud to say that I have made a lot of changes in my life since my last drink. I am, however, still struggling with the same thing that I was struggling with when I was using: I am lonely.\n",
      "\t\t| I am very fortunate to live in a small town. I have a lot of friends here. My wife and I have been married for 12 years, and we have two beautiful children. We have good jobs. I have a good job, and I love it.\n",
      "\t\t| In spite of all of this, I still feel lonely. My wife works a lot, and I have to spend a lot of time with the kids. I have to get up really early to get them ready for school, and I am often exhausted by the time they come home from school. I have no problem with this. I love them. I just wish that I had some time with them when they were not occupied with schoolwork, homework, etc.\n",
      "\t\t| I know that this is something that a lot of people go through. I am not the only person who is lonely, even in this small town.\n",
      "\n",
      "LLM's answer: Hello, my name is William, and I am a recovering alcoholic. I have been clean and sober for ten years, and I am very proud of that fact. I am also proud to say that I have made a lot of changes in my life since my last drink. I am, however, still struggling with the same thing that I was struggling with when I was using: I am lonely.\n",
      "I am very fortunate to live in a small town. I have a lot of friends here. My wife and I have been married for 12 years, and we have two beautiful children. We have good jobs. I have a good job, and I love it.\n",
      "In spite of all of this, I still feel lonely. My wife works a lot, and I have to spend a lot of time with the kids. I have to get up really early to get them ready for school, and I am often exhausted by the time they come home from school. I have no problem with this. I love them. I just wish that I had some time with them when they were not occupied with schoolwork, homework, etc.\n",
      "I know that this is something that a lot of people go through. I am not the only person who is lonely, even in this small town.\n"
     ]
    }
   ],
   "source": [
    "from langchain_helpers import OpenLlamaLLM, DebugCallbackHandler\n",
    "\n",
    "debug_handler = DebugCallbackHandler()\n",
    "llm = OpenLlamaLLM(host=\"127.0.0.1\", port=8000, model_name=\"llama_7b\", callbacks=[debug_handler])\n",
    "\n",
    "# llm(\"Code a python function to add two numbers.\")\n",
    "answer = llm(\"Hello, my name is \")\n",
    "print(f\"\\nLLM's answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
