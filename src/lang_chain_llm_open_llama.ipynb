{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain: LLM Class for Open Llama server\n",
    "\n",
    "This is an example on how to create a LLM class for our own Open Llama server.\n",
    "\n",
    "**Note:**\n",
    "This assumes the server is running using the example API (in this same code repository) on a server running on our own EC2 instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Any, List, Mapping, Optional\n",
    "\n",
    "import requests\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.llms.base import LLM\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.callbacks.manager import CallbackManagerForLLMRun\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from lchain.debug_handler import DebugCallbackHandler\n",
    "from lchain.utils import read_openai_key\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "read_openai_key()\n",
    "debug_handler = DebugCallbackHandler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenLLama LLM Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.api import CompletionRequest\n",
    "\n",
    "# Local test\n",
    "HOST = \"127.0.0.1\"\n",
    "PORT = 8000\n",
    "MODEL_NAME = 'fake'\n",
    "\n",
    "# Uncomment these lines and use your own server's IP address\n",
    "# HOST = '10.101.102.103'\n",
    "# PORT = 80\n",
    "# MODEL_NAME = 'llama_7b'\n",
    "\n",
    "URL = f\"http://{HOST}:{PORT}/v1/completions/\"\n",
    "\n",
    "class OpenLlamaLLM(LLM):\n",
    "    model_name = MODEL_NAME\n",
    "    url = URL\n",
    "    \"\"\"Server URL\"\"\"\n",
    "    temperature: float = 0.7\n",
    "    \"\"\"What sampling temperature to use.\"\"\"\n",
    "    max_tokens: int = 256\n",
    "    \"\"\"The maximum number of tokens to generate in the completion.\n",
    "    -1 returns as many tokens as possible given the prompt and\n",
    "    the models maximal context size.\"\"\"\n",
    "    top_p: float = 1\n",
    "    \"\"\"Penalizes repeated tokens.\"\"\"\n",
    "    n: int = 1\n",
    "    \"\"\"Adjust the probability of specific tokens being generated.\"\"\"\n",
    "    request_timeout = 120\n",
    "    \"\"\"The maximum number of seconds to wait for the server to respond.\"\"\"\n",
    "    max_retries: int = 3\n",
    "    \"\"\"The maximum number of retries before giving up.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        return \"open_llama_fastapi_server\"\n",
    "\n",
    "    def _completion_request(self, prompt: str) -> CompletionRequest:\n",
    "        \"\"\" Create a CompletionRequest object from the current parameters \"\"\"\n",
    "        return { 'model': self.model_name\n",
    "                , 'prompt': prompt\n",
    "                , 'suffix': ''\n",
    "                , 'max_tokens': self.max_tokens\n",
    "                , 'temperature': self.temperature\n",
    "                , 'top_p': self.top_p\n",
    "                , 'n': self.n\n",
    "                , 'logprobs': 1\n",
    "                , 'echo': False\n",
    "                , 'stop': []\n",
    "                , 'presence_penalty': 0\n",
    "                , 'frequency_penalty': 0\n",
    "                , 'best_of': 1\n",
    "                , 'logit_bias': {}\n",
    "                }\n",
    "\n",
    "    def _call(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        stop: Optional[List[str]] = None,\n",
    "        run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "    ) -> str:\n",
    "        print(f\"OpenLlamaLLM prompt (len:{len(prompt)}): '{prompt}'\")\n",
    "        if stop is not None:\n",
    "            print(f\"OpenLlamaLLM, stop kwargs: {stop}\")\n",
    "        compl_req = self._completion_request(prompt)\n",
    "        if run_manager:\n",
    "            run_manager.on_llm_new_token(token=prompt)\n",
    "        r = requests.post(url=self.url, json=compl_req, timeout=self.request_timeout)\n",
    "        logger.debug(f\"OpenLlamaLLM response: {r}\")\n",
    "        r = r.json()\n",
    "        # Get first response\n",
    "        response = r['choices'][0]['text']\n",
    "        return response\n",
    "    \n",
    "    @property\n",
    "    def _identifying_params(self) -> Mapping[str, Any]:\n",
    "        \"\"\"Get the identifying parameters.\"\"\"\n",
    "        return {\"url\": self.url\n",
    "                , 'model': self.model_name\n",
    "                , 'max_tokens': self.max_tokens\n",
    "                , 'temperature': self.temperature\n",
    "                , 'top_p': self.top_p\n",
    "                , 'n': self.n\n",
    "                , 'max_retries': self.max_retries\n",
    "                , 'request_timeout': self.request_timeout\n",
    "                }\n",
    "    \n",
    "llm = OpenLlamaLLM(callbacks=[debug_handler])\n",
    "llm(\"Code a python function to add two numbers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm(\"Hello, my name is \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
