{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KrEyCR7fiVl"
      },
      "source": [
        "# LLMs from dummies - Part 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKHJhcCXfiVm"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "! pip install Levenshtein\n",
        "! pip install bpe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXTm1zwxfvdI",
        "outputId": "2369f195-206d-414e-cfda-04ff84b510e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting Levenshtein\n",
            "  Downloading Levenshtein-0.21.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (172 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.5/172.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rapidfuzz<4.0.0,>=2.3.0 (from Levenshtein)\n",
            "  Downloading rapidfuzz-3.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, Levenshtein\n",
            "Successfully installed Levenshtein-0.21.1 rapidfuzz-3.1.1\n",
            "Collecting bpe\n",
            "  Downloading bpe-1.0-py3-none-any.whl (6.8 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bpe) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpe) (4.65.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from bpe) (7.2.2)\n",
            "Collecting hypothesis (from bpe)\n",
            "  Downloading hypothesis-6.81.2-py3-none-any.whl (414 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m414.8/414.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from bpe) (0.12.0)\n",
            "Collecting mypy (from bpe)\n",
            "  Downloading mypy-1.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis->bpe) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis->bpe) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis->bpe) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy->bpe) (4.7.1)\n",
            "Collecting mypy-extensions>=1.0.0 (from mypy->bpe)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy->bpe) (2.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bpe) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bpe) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bpe) (2022.10.31)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->bpe) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->bpe) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->bpe) (1.2.0)\n",
            "Installing collected packages: mypy-extensions, hypothesis, mypy, bpe\n",
            "Successfully installed bpe-1.0 hypothesis-6.81.2 mypy-1.4.1 mypy-extensions-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyK-nbj8fiVn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "\n",
        "from Levenshtein import distance\n",
        "from bpe import Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32XTK4hgfiVu"
      },
      "outputs": [],
      "source": [
        "# Device for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "split = 'train'\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 64\n",
        "max_iters = 5000              # Maximum training iterations\n",
        "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
        "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
        "\n",
        "# Architecture parameters\n",
        "max_vocab_size = 256          # Maximum vocabulary size\n",
        "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
        "block_size = 16               # Context length for predictions\n",
        "n_embd = 32                   # Embedding size\n",
        "num_heads = 2                 # Number of head in multi-headed attention\n",
        "n_layer = 2                   # Number of Blocks\n",
        "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
        "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
        "\n",
        "head_size = n_embd // num_heads\n",
        "assert (num_heads * head_size) == n_embd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDDSf0vbfiVn"
      },
      "outputs": [],
      "source": [
        "def latex_matrix(a):\n",
        "    \"\"\"Returns a LaTeX matrix from a numpy array.\"\"\"\n",
        "    if len(a.shape) > 2:\n",
        "        raise ValueError('matrix can at most display two dimensions')\n",
        "    lines = str(a)\n",
        "    for s in ['tensor', '(', ')', '. ', '.,', ',', '[', '.]', ']']:\n",
        "        lines = lines.replace(s, '')\n",
        "    lines = lines.splitlines()\n",
        "    rv = [r'\\left[\\begin{matrix}']\n",
        "    rv += ['  ' + ' & '.join(l.split()) + r' \\\\' for l in lines]\n",
        "    rv +=  [r'\\end{matrix}\\right]']\n",
        "    return '\\n'.join(rv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1rQwCb4fiVn"
      },
      "source": [
        "## Program for literal translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ponq-ZqQfiVo"
      },
      "outputs": [],
      "source": [
        "dictionary = {\n",
        "    'le': 'the'\n",
        "    , 'chat': 'cat'\n",
        "    , 'est': 'is'\n",
        "    , 'sous': 'under'\n",
        "    , 'la': 'the'\n",
        "    , 'table': 'table'\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EGB9coXzfiVo"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "    ''' Split sentences into tokens (words) '''\n",
        "    return text.split()\n",
        "\n",
        "def translate(sentence):\n",
        "    ''' Translate a sentence '''\n",
        "    out = ''\n",
        "    for token in tokenize(sentence):\n",
        "        out += dictionary[token] + ' '\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "78IJrwNrfiVo",
        "outputId": "046bfd3a-7a29-47e9-9bc7-af9299d34826"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the cat is under the table '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "translate(\"le chat est sous la table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7oajssu_fiVo"
      },
      "source": [
        "### Improvement: What if the 'key' is not in the dictionary?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IxVzSJcJfiVo"
      },
      "outputs": [],
      "source": [
        "def find_closest_key(query):\n",
        "    ''' Find closest key in dictionary '''\n",
        "    closest_key, min_dist = None, float('inf')\n",
        "    for key in dictionary.keys():\n",
        "        dist = distance(query, key)\n",
        "        if dist < min_dist:\n",
        "            min_dist, closest_key = dist, key\n",
        "    return closest_key\n",
        "\n",
        "\n",
        "def translate(sentence):\n",
        "    ''' Translate a sentence '''\n",
        "    out = ''\n",
        "    for query in tokenize(sentence):\n",
        "        key = find_closest_key(query)\n",
        "        out += dictionary[key] + ' '\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "n7l_C9CPfiVp",
        "outputId": "f963e853-efea-4e57-ae95-094516590e18"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'table '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "translate(\"tables\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kmR3SHLfiVp"
      },
      "source": [
        "## Convert to Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_9BZlVNfiVp"
      },
      "source": [
        "### Define \"vocabularies\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3-h1SWT5fiVp",
        "outputId": "d936faec-2d28-4f02-c7af-75ea0d6e48ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary input (6): ['chat', 'est', 'la', 'le', 'sous', 'table']\n",
            "Vocabulary output (5): ['cat', 'is', 'table', 'the', 'under']\n"
          ]
        }
      ],
      "source": [
        "# Vocabulary: All the words in the dictionary\n",
        "vocabulary_in = sorted(list(set(dictionary.keys())))\n",
        "print(f\"Vocabulary input ({len(vocabulary_in)}): {vocabulary_in}\")\n",
        "\n",
        "vocabulary_out = sorted(list(set(dictionary.values())))\n",
        "print(f\"Vocabulary output ({len(vocabulary_out)}): {vocabulary_out}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZh_e0Y4fiVp"
      },
      "source": [
        "### Encode tokens using \"one hot\" encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whiuzFdbfiVp"
      },
      "outputs": [],
      "source": [
        "# Convert to one hot encoding\n",
        "def encode_one_hot(vocabulary):\n",
        "    vocabulary_size = len(vocabulary)\n",
        "    one_hot = dict()\n",
        "    LEN = len(vocabulary)\n",
        "    for i, key in enumerate(vocabulary):\n",
        "        one_hot_vector = torch.zeros(LEN)\n",
        "        one_hot_vector[i] = 1\n",
        "        one_hot[key] = one_hot_vector\n",
        "        print(f\"{key}\\t: {one_hot[key]}\")\n",
        "    return one_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCCkVxGCfiVp",
        "outputId": "16ed4e69-c750-4272-e210-d754977cd843"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "chat\t: tensor([1., 0., 0., 0., 0., 0.])\n",
            "est\t: tensor([0., 1., 0., 0., 0., 0.])\n",
            "la\t: tensor([0., 0., 1., 0., 0., 0.])\n",
            "le\t: tensor([0., 0., 0., 1., 0., 0.])\n",
            "sous\t: tensor([0., 0., 0., 0., 1., 0.])\n",
            "table\t: tensor([0., 0., 0., 0., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "one_hot_in = encode_one_hot(vocabulary_in)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m0VW7sxefiVp"
      },
      "outputs": [],
      "source": [
        "# # Show vectors of one hot encoded tokens\n",
        "\n",
        "# for k, v in one_hot_in.items():\n",
        "#     print(\"$$ E_{\", k ,\"} = \" , latex_matrix(v), \"$$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1E13MoHfiVp"
      },
      "source": [
        "$$ E_{ chat } =  \\left[\\begin{matrix}\n",
        "  1 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "\\end{matrix}\\right] $$\n",
        "$$ E_{ est } =  \\left[\\begin{matrix}\n",
        "  0 & 1 & 0 & 0 & 0 & 0\\\\\n",
        "\\end{matrix}\\right] $$\n",
        "$$ E_{ la } =  \\left[\\begin{matrix}\n",
        "  0 & 0 & 1 & 0 & 0 & 0\\\\\n",
        "\\end{matrix}\\right] $$\n",
        "$$ E_{ le } =  \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 1 & 0 & 0\\\\\n",
        "\\end{matrix}\\right] $$\n",
        "$$ E_{ sous } =  \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 0 & 1 & 0\\\\\n",
        "\\end{matrix}\\right] $$\n",
        "$$ E_{ table } =  \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 0 & 0 & 1\\\\\n",
        "\\end{matrix}\\right] $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48L9xDUffiVp",
        "outputId": "ef25e284-922d-4c77-e229-c07d122ee591"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\t: tensor([1., 0., 0., 0., 0.])\n",
            "is\t: tensor([0., 1., 0., 0., 0.])\n",
            "table\t: tensor([0., 0., 1., 0., 0.])\n",
            "the\t: tensor([0., 0., 0., 1., 0.])\n",
            "under\t: tensor([0., 0., 0., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "# Same for output vocabulary\n",
        "one_hot_out = encode_one_hot(vocabulary_out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmoK91lifiVq"
      },
      "source": [
        "### Let's create a 'dictionary' using matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P5q3LN9BfiVq",
        "outputId": "85136a67-ad55-47f6-eaa1-1f142eccc5d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 1., 0., 0.],\n",
              "        [1., 0., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "K = torch.stack( [one_hot_in[k] for k in dictionary.keys()] )\n",
        "K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGj63qGbfiVq",
        "outputId": "e31e8003-b327-46f9-e495-bcff5b2a1079"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0., 0., 1., 0.],\n",
              "        [1., 0., 0., 0., 0.],\n",
              "        [0., 1., 0., 0., 0.],\n",
              "        [0., 0., 0., 0., 1.],\n",
              "        [0., 0., 0., 1., 0.],\n",
              "        [0., 0., 1., 0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "V = torch.stack( [one_hot_out[k] for k in dictionary.values()] )\n",
        "V"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlFIpyJNfiVq",
        "outputId": "3b1448cc-4544-4ba9-b46d-ea2613f95ad0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query token     :  tensor([0., 0., 0., 0., 1., 0.])\n",
            "Select key (K)  :  tensor([0., 0., 0., 1., 0., 0.])\n",
            "Select value (V):  tensor([0., 0., 0., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "# Example of looking for a query string in a dictionary\n",
        "q = one_hot_in['sous']\n",
        "print(\"Query token     : \", q)\n",
        "print(\"Select key (K)  : \", q @ K.T)\n",
        "print(\"Select value (V): \", q @ K.T @ V)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pEBS6-mSfiVq"
      },
      "source": [
        "Query vector, K matrix, and V matrix:\n",
        "\n",
        "$$\n",
        "q = \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 0 & 1 & 0\\\\\n",
        "\\end{matrix}\\right]\n",
        ";\n",
        "K = \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 1 & 0 & 0\\\\\n",
        "  1 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 1 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 1 & 0\\\\\n",
        "  0 & 0 & 1 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 1\\\\\n",
        "\\end{matrix}\\right]\n",
        ";\n",
        "V = \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 1 & 0\\\\\n",
        "  1 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 1 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 1\\\\\n",
        "  0 & 0 & 0 & 1 & 0\\\\\n",
        "  0 & 0 & 1 & 0 & 0\\\\\n",
        "\\end{matrix}\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckLcX0StfiVq"
      },
      "source": [
        "The operation $q . K^T . V$ allows us to build a dictionary-like structure from a set of vectors\n",
        "\n",
        "This is an example on how to select the value from a query:\n",
        "\n",
        "$$\n",
        "q . K^T . V =\n",
        "\\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 0 & 1 & 0\\\\\n",
        "\\end{matrix}\\right]\n",
        ".\n",
        "\\left[\\begin{matrix}\n",
        "  0 & 1 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 1 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 1 & 0\\\\\n",
        "  1 & 0 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 1 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 1\\\\\n",
        "\\end{matrix}\\right]\n",
        ".\n",
        "\\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 1 & 0\\\\\n",
        "  1 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 1 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 1\\\\\n",
        "  0 & 0 & 0 & 1 & 0\\\\\n",
        "  0 & 0 & 1 & 0 & 0\\\\\n",
        "\\end{matrix}\\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFg1VMSsfiVq"
      },
      "source": [
        "$$\n",
        "q . K^T . V =\n",
        "\\hspace{2cm}\n",
        "\\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 1 & 0 & 0\\\\\n",
        "\\end{matrix}\\right]\n",
        "\\hspace{1.5cm}\n",
        ".\n",
        "\\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 1 & 0\\\\\n",
        "  1 & 0 & 0 & 0 & 0\\\\\n",
        "  0 & 1 & 0 & 0 & 0\\\\\n",
        "  0 & 0 & 0 & 0 & 1\\\\\n",
        "  0 & 0 & 0 & 1 & 0\\\\\n",
        "  0 & 0 & 1 & 0 & 0\\\\\n",
        "\\end{matrix}\\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ85b39kfiVq"
      },
      "source": [
        "$$\n",
        "q . K^T . V\n",
        "=\n",
        "\\hspace{3.5cm}\n",
        "\\left[\\begin{matrix}\n",
        "0 & 0 & 0 & 0 & 1\\\\\n",
        "\\end{matrix}\\right]\n",
        "\\hspace{3.5cm}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ23D9uhfiVq"
      },
      "source": [
        "### Decode one hot vector to a token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VjZz2XCJfiVq"
      },
      "outputs": [],
      "source": [
        "def decode_one_hot(one_hot, vector):\n",
        "    \"\"\" Decode \"one hot\". Find the best matching 'token' \"\"\"\n",
        "    best_key, best_cosine_sim = None, 0\n",
        "    for k, v in one_hot.items():\n",
        "        cosine_sim = torch.dot(vector, v)   # Since the vectors are normalized, this is the same as \"cosine similarity\"\n",
        "        if cosine_sim > best_cosine_sim:\n",
        "            best_cosine_sim = cosine_sim\n",
        "            best_key = k\n",
        "    return best_key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsL04IN4fiVq"
      },
      "source": [
        "### Now we have a translate function using matrices an vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNVzdmH0fiVr"
      },
      "outputs": [],
      "source": [
        "def translate(sentence):\n",
        "    sentence_out = ''\n",
        "    for token_in in tokenize(sentence):\n",
        "        q = one_hot_in[token_in]\n",
        "        out = q @ K.T @ V\n",
        "        token_out = decode_one_hot(one_hot_out, out)\n",
        "        sentence_out += token_out + ' '\n",
        "    return sentence_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "spfxPttYfiVr",
        "outputId": "9bae8193-2cb3-47d0-91f3-37d91b755416"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the cat is under the table '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "# Let's check that it works:\n",
        "translate(\"le chat est sous la table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiADkAHvfiVr"
      },
      "source": [
        "### ... a few more tweaks towards \"Attention\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRn-IgolfiVr"
      },
      "source": [
        "### Similar tokens => similar vectors: Adding a softmax"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi2oYEBJfiVr"
      },
      "outputs": [],
      "source": [
        "# print('E_{table} = ', latex_matrix(one_hot_in['table']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAvqpMFnfiVr"
      },
      "source": [
        "$$\n",
        "E_{table} =  \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 0 & 0 & 1\\\\\n",
        "\\end{matrix}\\right]\n",
        "$$\n",
        "\n",
        "$$\n",
        "E_{tables} =  \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 0 & 0 & 0.95\\\\\n",
        "\\end{matrix}\\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToidOVDefiVr"
      },
      "source": [
        "Our new equation is:\n",
        "$$\n",
        "softmax(q . K^T) . V\n",
        "$$\n",
        "\n",
        "We adjust using by the dimensionality of the query vector, and we get:\n",
        "\n",
        "$$\n",
        "softmax\\left( \\frac{q . K^T}{\\sqrt{d}} \\right) . V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "JNhdFNddfiVr",
        "outputId": "e5ffd060-2b4d-4ad1-c352-ed8e568b9f7d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the cat is under the table '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "def translate(sentence):\n",
        "    \"\"\" Translate using K and V matrices \"\"\"\n",
        "    sentence_out = ''\n",
        "    for token_in in tokenize(sentence):\n",
        "        q = one_hot_in[token_in]\n",
        "        out = torch.softmax(q @ K.T, 0) @ V\n",
        "        token_out = decode_one_hot(one_hot_out, out)\n",
        "        sentence_out += token_out + ' '\n",
        "    return sentence_out\n",
        "\n",
        "translate(\"le chat est sous la table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSJd8Vc1fiVr"
      },
      "source": [
        "### Improvement: All queries in parallel. The \"Q\" matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-yJ9mgWDfiVr"
      },
      "outputs": [],
      "source": [
        "sentence = \"le chat est sous la table\"\n",
        "\n",
        "Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n",
        "# print(latex_matrix(Q))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjmHlV3bfiVs"
      },
      "source": [
        "$$\n",
        "Q = \\left[\\begin{matrix}\n",
        "  0 & 0 & 0 & 1 & 0 & 0 \\\\\n",
        "  1 & 0 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 1 & 0 & 0 & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & 0 & 1 & 0 \\\\\n",
        "  0 & 0 & 1 & 0 & 0 & 0 \\\\\n",
        "  0 & 0 & 0 & 0 & 0 & 1 \\\\\n",
        "\\end{matrix}\\right]\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqKKz2qOfiVs"
      },
      "source": [
        "$$\n",
        "Attention(Q, K, V) = softmax\\left( \\frac{Q . K^T}{\\sqrt{d}} \\right) V\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "da1vmuqMfiVs",
        "outputId": "944dbd04-3047-4f8b-f058-122a1163da2c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the cat is under the table'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "def translate(sentence):\n",
        "    \"\"\" Translate using a single matrix multiplication instead of a 'for' loop. \"\"\"\n",
        "    Q = torch.stack([one_hot_in[token] for token in tokenize(sentence)])\n",
        "    out = torch.softmax(Q @ K.T, 0) @ V\n",
        "    return ' '.join([decode_one_hot(one_hot_out, o) for o in out])\n",
        "\n",
        "translate(\"le chat est sous la table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2Ywg2QtfiVs"
      },
      "source": [
        "### Making attention more powerful"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1-_xrr3fiVs"
      },
      "source": [
        "$$\n",
        "Attention(Q, K, V)  => Attention(Q . W^Q, K . W^K, V . W^V)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOy69RxhfiVs"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5AWBg_tfiVs"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" Self attention head \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "    # Attention score\n",
        "    w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # Query * Keys / normalization\n",
        "    w = F.softmax(w, dim=-1)  # Do a softmax across the last dimesion\n",
        "    # Add weighted values\n",
        "    out = w @ v\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5TZTDAIfiVs"
      },
      "source": [
        "## Improving tokenization: BPE\n",
        "\n",
        "Example: Tokenizing the \"Shakespare\" dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnzgfoxjfiVs"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: Downloads the datasets from '' to the `datasets` directory\n",
        "datasets_dir = Path(\".\")\n",
        "shakespeare_data = datasets_dir / \"shakespeare.txt\"\n",
        "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not shakespeare_data.exists():\n",
        "    with open(shakespeare_data, 'w') as f:\n",
        "        f.write(requests.get(shakespeare_url).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v_AY6snfiVs",
        "outputId": "eeef84f9-6623-4c70-b953-ffa5f308322d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us...\n"
          ]
        }
      ],
      "source": [
        "# Load the file\n",
        "with open(shakespeare_data, \"r\") as f:\n",
        "    text = f.read()\n",
        "    print(text[:300] + \"...\")\n",
        "\n",
        "encoder = Encoder()  # Using default parameters: vocab_size=8192\n",
        "encoder.fit(text.split('\\n'))  # Fitting the model: i.e. using the data to get the translation table"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Show first items in the BPE vocabulary\n",
        "', '.join( [f\"'{k}' : {v}\" for k, v in encoder.bpe_vocab.items()][:100] )"
      ],
      "metadata": {
        "id": "Z_slDum3Ckhy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "d164e66c-658f-4b5a-96da-477d4038be8d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"'__sow' : 6553, '__eow' : 6554, 'e' : 6555, 's' : 6556, 'i' : 6557, 'r' : 6558, 'n' : 6559, 't' : 6560, 'a' : 6561, 'o' : 6562, 'l' : 6563, 'd' : 6564, 'c' : 6565, 'u' : 6566, 'g' : 6567, 'p' : 6568, 'h' : 6569, 'm' : 6570, 'in' : 6571, 'er' : 6572, 'es' : 6573, 'b' : 6574, 'ed' : 6575, 'f' : 6576, 'ng' : 6577, 'y' : 6578, 're' : 6579, 'st' : 6580, 'en' : 6581, 'te' : 6582, 'w' : 6583, 'v' : 6584, 'le' : 6585, 'ti' : 6586, 'on' : 6587, 'nt' : 6588, 'ar' : 6589, 'an' : 6590, 'un' : 6591, 'k' : 6592, 'co' : 6593, 'ri' : 6594, 'is' : 6595, 'de' : 6596, 'at' : 6597, 'ra' : 6598, 'or' : 6599, 'se' : 6600, 'li' : 6601, 'ne' : 6602, 'he' : 6603, 'ou' : 6604, 've' : 6605, 'ss' : 6606, 'di' : 6607, 'al' : 6608, 'it' : 6609, 'ea' : 6610, 'ns' : 6611, 'th' : 6612, 'ro' : 6613, 'pe' : 6614, 'ur' : 6615, 'et' : 6616, 'ly' : 6617, 'el' : 6618, 'me' : 6619, 'ta' : 6620, 'la' : 6621, 'rs' : 6622, 'io' : 6623, 'ch' : 6624, 'nd' : 6625, 'ce' : 6626, 'us' : 6627, 'll' : 6628, 'tr' : 6629, 'sh' : 6630, 'ie' : 6631, 'ts' : 6632, 'nc' : 6633, 'as' : 6634, 'pr' : 6635, 'ma' : 6636, 'si' : 6637, 'ge' : 6638, 'mi' : 6639, 'il' : 6640, 'be' : 6641, 'lo' : 6642, 'rt' : 6643, 'ca' : 6644, 'ic' : 6645, 'bl' : 6646, 'ac' : 6647, 'ha' : 6648, 'ec' : 6649, 'ai' : 6650, 'sp' : 6651, 'em' : 6652\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "eKHJhcCXfiVm",
        "D1rQwCb4fiVn",
        "9kmR3SHLfiVp",
        "a5TZTDAIfiVs",
        "aj199eY3fiVt",
        "_lb6N0J_HAFe",
        "rlJzuC3k6TPd",
        "9MbXPG55fiVu",
        "qCMm-s-rfiVv",
        "MBArcqEgfiVv",
        "_mEmlh7LfiVv",
        "hsQnrMxyYLu6",
        "hTBRxMZWkzuu",
        "zwSjNT4FdlC1",
        "qmnaLp1njUxW",
        "RdrozxXsmOrQ",
        "-J8-0YoXo5gJ"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}