{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8KrEyCR7fiVl"
      },
      "source": [
        "# LLMs from dummies - Part 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKHJhcCXfiVm"
      },
      "source": [
        "## Initialize"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install packages\n",
        "! pip install Levenshtein\n",
        "! pip install bpe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FXTm1zwxfvdI",
        "outputId": "3d93d3b1-a08b-420f-b449-c9d85196be19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Levenshtein in /usr/local/lib/python3.10/dist-packages (0.21.1)\n",
            "Requirement already satisfied: rapidfuzz<4.0.0,>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from Levenshtein) (3.1.1)\n",
            "Requirement already satisfied: bpe in /usr/local/lib/python3.10/dist-packages (1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from bpe) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from bpe) (4.65.0)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from bpe) (7.2.2)\n",
            "Requirement already satisfied: hypothesis in /usr/local/lib/python3.10/dist-packages (from bpe) (6.81.1)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from bpe) (0.12.0)\n",
            "Requirement already satisfied: mypy in /usr/local/lib/python3.10/dist-packages (from bpe) (1.4.1)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis->bpe) (23.1.0)\n",
            "Requirement already satisfied: sortedcontainers<3.0.0,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis->bpe) (2.4.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from hypothesis->bpe) (1.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy->bpe) (4.7.1)\n",
            "Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from mypy->bpe) (1.0.0)\n",
            "Requirement already satisfied: tomli>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from mypy->bpe) (2.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->bpe) (8.1.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->bpe) (1.3.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->bpe) (2022.10.31)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->bpe) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->bpe) (23.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->bpe) (1.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GyK-nbj8fiVn"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import time\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import requests\n",
        "\n",
        "from Levenshtein import distance\n",
        "from bpe import Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "32XTK4hgfiVu"
      },
      "outputs": [],
      "source": [
        "# Device for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "split = 'train'\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 64\n",
        "max_iters = 5000              # Maximum training iterations\n",
        "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
        "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
        "\n",
        "# Architecture parameters\n",
        "max_vocab_size = 256          # Maximum vocabulary size\n",
        "vocab_size = max_vocab_size   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
        "block_size = 16               # Context length for predictions\n",
        "n_embd = 32                   # Embedding size\n",
        "num_heads = 2                 # Number of head in multi-headed attention\n",
        "n_layer = 2                   # Number of Blocks\n",
        "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
        "dropout = 0.0                 # Normalization using dropout# 10.788929 M parameters\n",
        "\n",
        "head_size = n_embd // num_heads\n",
        "assert (num_heads * head_size) == n_embd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YOy69RxhfiVs"
      },
      "source": [
        "## Attention Head"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5AWBg_tfiVs"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" Self attention head \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "    # Attention score\n",
        "    w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5         # Query * Keys / normalization\n",
        "    w = F.softmax(w, dim=-1)                                # Do a softmax across the last dimesion\n",
        "    # Add weighted values\n",
        "    out = w @ v\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5TZTDAIfiVs"
      },
      "source": [
        "## Download 'Shakespeare' dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnzgfoxjfiVs"
      },
      "outputs": [],
      "source": [
        "# IMPORTANT: Downloads the datasets from '' to the `datasets` directory\n",
        "datasets_dir = Path(\".\")\n",
        "shakespeare_data = datasets_dir / \"shakespeare.txt\"\n",
        "shakespeare_url = 'https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt'\n",
        "\n",
        "if not shakespeare_data.exists():\n",
        "    with open(shakespeare_data, 'w') as f:\n",
        "        f.write(requests.get(shakespeare_url).text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v_AY6snfiVs",
        "outputId": "52c24a63-572e-4d38-f0c4-4a889cdfc9c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n",
            "All:\n",
            "We know't, we know't.\n",
            "\n",
            "First Citizen:\n",
            "Let us...\n"
          ]
        }
      ],
      "source": [
        "# Load the file\n",
        "with open(shakespeare_data, \"r\") as f:\n",
        "    text = f.read()\n",
        "    print(text[:300] + \"...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj199eY3fiVt"
      },
      "source": [
        "## Language Model 1: Our first language model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nsbz95tfiVu"
      },
      "outputs": [],
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\" Multi-headed attention model \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)   # Convert input hot-encoded words to vectors of n_embd dimensions\n",
        "    self.head = Head()                                              # Attention head\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)                    # Convert head's output (a word represneted as an embedding) to a probability vector of vocab_size dimensions\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    x = self.token_embedding_table(idx)\n",
        "    x = self.head(x)\n",
        "    logits = self.lm_head(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      # Calculate loss\n",
        "      b, t, c = logits.shape                                        # b: batch_size, t: sequence length (block_size), c: number of classes (vocab_size)\n",
        "      logits = logits.view(b*t, c)\n",
        "      targets = targets.view(b*t)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Boilerplate code to train and use the model"
      ],
      "metadata": {
        "id": "_lb6N0J_HAFe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We need some basic code to train and use the model:\n",
        "- A \"tokenizer\" to help us encode\n",
        "- A \"Dataset\"\n",
        "- A \"train loop\" to train the model\n",
        "- A \"generate\" function to generate model's output from a prompt"
      ],
      "metadata": {
        "id": "yUiqUW3gd7r_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenizer"
      ],
      "metadata": {
        "id": "rlJzuC3k6TPd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrivialTokenizer:\n",
        "  \"\"\" Trivial tokenizer: Converts to chars \"\"\"\n",
        "\n",
        "  def decode(self, tokens_encoded):\n",
        "    return ''.join([self.itos[i.item()] for i in tokens_encoded])\n",
        "\n",
        "  def encode(self, text):\n",
        "    encoded_tokens = [self.stoi[c] for c in text]\n",
        "    return torch.tensor(encoded_tokens, dtype=torch.long)             # Convert to torch tensor\n",
        "\n",
        "  def train(self, text):\n",
        "    chars = sorted(list(set(text)))\n",
        "    self.vocab_size = len(chars)\n",
        "    self.stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "    self.itos = {i:ch for i,ch in enumerate(chars)}\n",
        "\n",
        "  def vocabulary_size(self):\n",
        "    return self.vocab_size"
      ],
      "metadata": {
        "id": "iczs-Fh_7DQP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TrivialTokenizer()\n",
        "tokenizer.train(text)\n",
        "print(tokenizer.encode('hi there'))\n",
        "print(tokenizer.decode(tokenizer.encode('hi there')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4d8pSsh7wqC",
        "outputId": "30e7e557-8713-4b43-ef84-12ea6e4c152a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([46, 47,  1, 58, 46, 43, 56, 43])\n",
            "hi there\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BpeTokenizer:\n",
        "  \"\"\" BPE Tokenizer \"\"\"\n",
        "\n",
        "  def __init__(self, max_vocab_size):\n",
        "    self.max_vocab_size = max_vocab_size\n",
        "    self._bpe_tokenizer = None\n",
        "\n",
        "  def decode(self, tokens_encoded):\n",
        "    return list(self._bpe_tokenizer.inverse_transform(tokens_encoded.tolist()))\n",
        "\n",
        "  def encode(self, text):\n",
        "    encoded_tokens = []\n",
        "    for sentence in text.split(\"\\n\"):\n",
        "      for tokens in self._bpe_tokenizer.transform(sentence):\n",
        "        encoded_tokens.extend(tokens)\n",
        "    return torch.tensor(encoded_tokens, dtype=torch.long)  # Convert to torch tensor\n",
        "\n",
        "  def train(self, text):\n",
        "    self._bpe_tokenizer = Encoder(vocab_size=self.max_vocab_size)\n",
        "    self._bpe_tokenizer.fit(text.split(\"\\n\"))\n",
        "\n",
        "  def vocabulary_size(self):\n",
        "    return len(self._bpe_tokenizer.bpe_vocab) + len(self._bpe_tokenizer.word_vocab)\n"
      ],
      "metadata": {
        "id": "h5fLUq_aHikx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MbXPG55fiVu"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tm5VaC3UfiVu"
      },
      "outputs": [],
      "source": [
        "class TextDataset:\n",
        "  \"\"\" Create a 'text' dataset for training and testing. \"\"\"\n",
        "  def __init__(self, file_name, cut = 0.8, split='train'):\n",
        "    self.file_name = file_name\n",
        "    self.cut = cut              # Percentage for training / validation\n",
        "    self.data = None            # Tokenized text data\n",
        "    self.data_train = None      # Training data split\n",
        "    self.data_validation = None # Validation data split\n",
        "    self.text = None            # Raw text data\n",
        "\n",
        "  def get_batch(self, split):\n",
        "    \"\"\" Create a batch of data from either the train or validation split \"\"\"\n",
        "    data = self.data_train if split == 'train' else self.data_validation\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))                   # Create random index for every sample in the batch\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "  def load(self):\n",
        "    \"\"\" Read dataset (i.e. text file) \"\"\"\n",
        "    with open(self.file_name, 'r', encoding='utf-8') as f:\n",
        "      self.text = f.read()\n",
        "    return self.text\n",
        "\n",
        "  def split(self):\n",
        "    \"\"\" Split dataset into training and validation \"\"\"\n",
        "    cut_len = int(self.cut * len(self.data))\n",
        "    self.data_train = self.data[:cut_len]\n",
        "    self.data_validation = self.data[cut_len:]\n",
        "\n",
        "  def tokenize(self):\n",
        "    \"\"\" Tokenize the text data \"\"\"\n",
        "    self.data = tokenizer.encode(self.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qa0nTQf4fiVv",
        "outputId": "287d9fdf-fa45-43ae-fa1d-3e8564af53da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocab_size: 65\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[39, 56, 11,  ..., 59, 43,  1],\n",
              "         [58,  1, 58,  ..., 59, 41, 46],\n",
              "         [43,  1, 58,  ..., 43, 47, 52],\n",
              "         ...,\n",
              "         [46, 47, 52,  ..., 52, 45, 57],\n",
              "         [59,  1, 54,  ..., 52,  8,  0],\n",
              "         [39, 52, 49,  ...,  1, 63, 43]], device='cuda:0'),\n",
              " tensor([[56, 11,  1,  ..., 43,  1, 50],\n",
              "         [ 1, 58, 46,  ..., 41, 46,  1],\n",
              "         [ 1, 58, 53,  ..., 47, 52, 45],\n",
              "         ...,\n",
              "         [47, 52, 49,  ..., 45, 57,  6],\n",
              "         [ 1, 54, 56,  ...,  8,  0,  0],\n",
              "         [52, 49,  1,  ..., 63, 43, 58]], device='cuda:0'))"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenizer = TrivialTokenizer()\n",
        "\n",
        "# Create a text dataset\n",
        "dataset = TextDataset(shakespeare_data)\n",
        "text = dataset.load()\n",
        "\n",
        "# Train tokenizer\n",
        "tokenizer.train(text)\n",
        "vocab_size = tokenizer.vocabulary_size()\n",
        "print(f\"vocab_size: {vocab_size}\")\n",
        "\n",
        "# Tokenize & split the dataset\n",
        "dataset.tokenize()\n",
        "dataset.split()\n",
        "\n",
        "# Example of getting a batch\n",
        "dataset.get_batch('train')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset.text[:50])\n",
        "dataset.data[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MH6Ec2_7IFnV",
        "outputId": "def8b366-b3b1-4bdb-afdb-938e7f6b4563"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([18, 47, 56, 57, 58,  1, 15, 47, 58, 47, 64, 43, 52, 10,  0, 14, 43, 44,\n",
              "        53, 56, 43,  1, 61, 43,  1, 54, 56, 53, 41, 43, 43, 42,  1, 39, 52, 63,\n",
              "         1, 44, 59, 56, 58, 46, 43, 56,  6,  1, 46, 43, 39, 56])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qCMm-s-rfiVv"
      },
      "source": [
        "### Estimating the model's performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A9fAkDxfiVv"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss(model, dataset):\n",
        "    \"\"\" Estimate 'train' and 'validation' losses using a few batches (eval_iters) \"\"\"\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = dataset.get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBArcqEgfiVv"
      },
      "source": [
        "### Training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KuENCgstfiVv"
      },
      "outputs": [],
      "source": [
        "def train(model, dataset):\n",
        "    # Train the model\n",
        "    model.train()\n",
        "    num_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Training model: {num_params/1e6}M parameters\", flush=True)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Create a training loop\n",
        "    loss_min = 1e9\n",
        "    for step in range(max_iters):\n",
        "        # Show loss at the beginning and end of the loop\n",
        "        if step % eval_interval == 0 or step == max_iters - 1:\n",
        "            losses = estimate_loss(model, dataset)\n",
        "            print(f\"Step {step}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\", flush=True)\n",
        "        # Sample batch data\n",
        "        xb, yb = dataset.get_batch('train')\n",
        "        # Evaluate model\n",
        "        logits, loss = model(xb, yb)\n",
        "        # Learn\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(\"Done training\", flush=True)\n",
        "    model.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_mEmlh7LfiVv"
      },
      "source": [
        "### Generate text using the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW_3UJTWfiVv"
      },
      "outputs": [],
      "source": [
        "def generate(model, dataset, prompt=None, max_new_tokens=500):\n",
        "    \"\"\" Run a generation and show the output \"\"\"\n",
        "    model.eval()\n",
        "    # Create a 'prompt'\n",
        "    if prompt is None:\n",
        "        prompt_encoded = torch.zeros((1, 1), dtype=torch.long)\n",
        "    else:\n",
        "        prompt_encoded = tokenizer.encode(prompt).unsqueeze(0)\n",
        "    # Run the model on the prompt, predicting one word at a time\n",
        "    tokens = []\n",
        "    for _ in range(max_new_tokens):\n",
        "        # Prepare the model's input\n",
        "        prompt_encoded = prompt_encoded.to(device)\n",
        "        prompt_encoded_crop = prompt_encoded[:, -block_size:]                     # Crop the prompt to only have 'block_size' inputs\n",
        "        # Use the model to predict the next token\n",
        "        logits, _ = model(prompt_encoded_crop)                                    # Predict\n",
        "        logits = logits[:, -1, :]                                                 # Use last time step, shape=(b, c)\n",
        "        probs = F.softmax(logits, dim=-1)                                         # Apply softmax to get token from vocabulaty, shape=(b, c)\n",
        "        next_token_encoded = torch.multinomial(probs, num_samples=1)              # Sample from the probability, shape (b, 1)\n",
        "        # Decode and update output tokens\n",
        "        print(tokenizer.decode(next_token_encoded), end='', flush=True)\n",
        "        # Update the prompt by appending the next token\n",
        "        prompt_encoded = torch.cat((prompt_encoded, next_token_encoded), dim=1)   # Concatenate predictions, shape=(b, t+1)\n",
        "    return ''.join(tokens) # Join the tokens into a string"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Generating with the model"
      ],
      "metadata": {
        "id": "hsQnrMxyYLu6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lgo10xNKfiVv"
      },
      "outputs": [],
      "source": [
        "def train_and_generate(model):\n",
        "  model.to(device)\n",
        "  # Generate using an untrained model\n",
        "  print(f\"Before training:\\n---\")\n",
        "  generate(model, dataset, prompt=\"thou shall not\")\n",
        "  print(\"\\n---\\n\")\n",
        "  # Train\n",
        "  train(model, dataset)\n",
        "  # Generate using trained model\n",
        "  print(f\"After training:\\n---\")\n",
        "  generate(model, dataset, prompt=\"thou shall not\")\n",
        "  print(\"\\n---\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea305d11-b15d-4b18-e9aa-ed98eb8207b3",
        "id": "jKVV0zdiFB71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training:\n",
            "---\n",
            ".:EY \n",
            "eo'NWBvmP-y\n",
            "hojqoBhQwq.PCREUz3'un-ywV$!uiAn'sZGATIbk;ENFZVXj.Gh$cfjGFziani,uBxLjSM!azwUEfbfXaiqKWj:K'SrZQydc&:n!guKatX:t,bq\n",
            "&mXQr:oA$$3D-bUkuf!fpxJo:NV?QIMUxmZ-onoSc:yliV&alnC'bpIMaQEP\n",
            "Mh-Axi ktLqp?qkKJ&dR s,MMq;qMYJmKDXokDyFHj .&Wcr,BdiEiznwScZ-;m:$YFsGlSel3rlrBIyF;-nd':eRVqM:UvzlpA;GGO-eCxBJWqrP&kIfwqAEtku:QOx-Xk$QOFTHZN-jWApNRCuQaDMc&ZxrW,b?ykH$PRqXHCG\n",
            "t\n",
            "BOYMELDaoKKmVRFjnu;NcDsaQsJ3&3n:og'zpXQueoAruuh?iuN$,v'K-;JqU;o wNy!tIr?;?AOx\n",
            "TnotitEqgCh CR ILfe'pMbYm?b.pVSgBLaez;De;&eBfOE: uGU3vdr\n",
            "---\n",
            "\n",
            "Training model: 0.007297M parameters\n",
            "Step 0: train loss 4.1952, val loss 4.1933\n",
            "Step 200: train loss 3.1847, val loss 3.2086\n",
            "Step 400: train loss 3.0025, val loss 3.0233\n",
            "Step 600: train loss 2.8724, val loss 2.8922\n",
            "Step 800: train loss 2.7791, val loss 2.8071\n",
            "Step 1000: train loss 2.7300, val loss 2.7390\n",
            "Step 1200: train loss 2.6622, val loss 2.6754\n",
            "Step 1400: train loss 2.6069, val loss 2.6230\n",
            "Step 1600: train loss 2.5634, val loss 2.5809\n",
            "Step 1800: train loss 2.5123, val loss 2.5293\n",
            "Step 2000: train loss 2.4685, val loss 2.5068\n",
            "Step 2200: train loss 2.4460, val loss 2.4715\n",
            "Step 2400: train loss 2.4148, val loss 2.4366\n",
            "Step 2600: train loss 2.3882, val loss 2.4219\n",
            "Step 2800: train loss 2.3766, val loss 2.4103\n",
            "Step 3000: train loss 2.3602, val loss 2.3835\n",
            "Step 3200: train loss 2.3313, val loss 2.3778\n",
            "Step 3400: train loss 2.3116, val loss 2.3612\n",
            "Step 3600: train loss 2.2867, val loss 2.3461\n",
            "Step 3800: train loss 2.2792, val loss 2.3263\n",
            "Step 4000: train loss 2.2703, val loss 2.3181\n",
            "Step 4200: train loss 2.2559, val loss 2.3050\n",
            "Step 4400: train loss 2.2407, val loss 2.2901\n",
            "Step 4600: train loss 2.2328, val loss 2.2875\n",
            "Step 4800: train loss 2.2189, val loss 2.2710\n",
            "Step 4999: train loss 2.2155, val loss 2.2620\n",
            "Done training\n",
            "After training:\n",
            "---\n",
            "hhth s hth h hh hh hhe h h he h h' h h ho hoooooofooofofufofooooooufoufououfououffouttthuttthtwhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh\n",
            "---\n",
            "\n",
            "CPU times: user 34.7 s, sys: 606 ms, total: 35.3 s\n",
            "Wall time: 38.1 s\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "model = LanguageModel()\n",
        "train_and_generate(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Masked Attention: Otherwise, we are cheating..."
      ],
      "metadata": {
        "id": "hTBRxMZWkzuu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2F83C1CfiVv"
      },
      "outputs": [],
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" Self attention head \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    self.query = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    self.value = nn.Linear(n_embd, n_embd, bias=False)\n",
        "    # Attention mask template, i.e. lower triangular matrix\n",
        "    # Note: This is a buffer because it's not a learnable parameter\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))  # <--\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, t, c = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "    # Attention score\n",
        "    w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5                               # Query * Keys / normalization\n",
        "    w = w.masked_fill(self.tril[:t, :t] == 0, float('-inf') )                     # <-- Mask everythig before time 't'. Replace weight by -inf whenver the triangular matrix is zero\n",
        "    w = F.softmax(w, dim=-1)                                                      # Do a softmax across the last dimesion\n",
        "    # Weighted values\n",
        "    out = w @ v\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = LanguageModel()\n",
        "train_and_generate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90826a15-a014-4119-ffb9-767fd71f45b4",
        "id": "OpnGMZmchbbn"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training:\n",
            "---\n",
            ".A n'icHo\n",
            ",ilqWFP$?QuJmSpK?-CKC'lVw?-?'VWpzdDF\n",
            "MG?dfTB$v AJZQrbErW3IeD.,REeEPj:zcrsmXKng!cPNXpSfbGwWE.FByGgmfqeRkUO;iyCZASbQ;3!QoMWJ;H'NRRUW,tuAbekJ\n",
            "WC:cvR$gu?Z.gWwtZp&UGbvkgZolxdphsQye$dT$rAi.pHW3lrfMEUqWTJwHzTijvWolMXUalTcq\n",
            "yQI?Qvb3VPLev$N;txL\n",
            "WkfjlTHJYUVhLsfex.ZN.$osmQGRRQNTsfCgI.fm&ZsWk:llZ Tqt vJQbVWZedt3wYA-f,-Qrl!gilqm&Rzr,h,Mq-G'\n",
            "tOdqdpMni3WnNLMCq$aeTWs&cbv.aYNYk3Dx:b3&leUyUTPl!:ZDpmCcaJtnYEIpFhwuImG',x\n",
            "wB$sAyQpQtZwud n,wApfTVi!&uyuhrHjCpYQEIxEwrGkcs:, WlijKVuLumjniHgUUq MuWE?KGs:UQaBMjP\n",
            "---\n",
            "\n",
            "Training model: 0.007297M parameters\n",
            "Step 0: train loss 4.2211, val loss 4.2219\n",
            "Step 200: train loss 3.3365, val loss 3.3494\n",
            "Step 400: train loss 3.0721, val loss 3.0875\n",
            "Step 600: train loss 2.9089, val loss 2.9317\n",
            "Step 800: train loss 2.8013, val loss 2.8284\n",
            "Step 1000: train loss 2.7329, val loss 2.7499\n",
            "Step 1200: train loss 2.6698, val loss 2.6889\n",
            "Step 1400: train loss 2.6381, val loss 2.6531\n",
            "Step 1600: train loss 2.6062, val loss 2.6375\n",
            "Step 1800: train loss 2.5925, val loss 2.6092\n",
            "Step 2000: train loss 2.5783, val loss 2.5953\n",
            "Step 2200: train loss 2.5621, val loss 2.5847\n",
            "Step 2400: train loss 2.5457, val loss 2.5807\n",
            "Step 2600: train loss 2.5356, val loss 2.5628\n",
            "Step 2800: train loss 2.5291, val loss 2.5584\n",
            "Step 3000: train loss 2.5267, val loss 2.5534\n",
            "Step 3200: train loss 2.5204, val loss 2.5464\n",
            "Step 3400: train loss 2.5000, val loss 2.5405\n",
            "Step 3600: train loss 2.5067, val loss 2.5328\n",
            "Step 3800: train loss 2.5003, val loss 2.5283\n",
            "Step 4000: train loss 2.4938, val loss 2.5209\n",
            "Step 4200: train loss 2.4921, val loss 2.5221\n",
            "Step 4400: train loss 2.4791, val loss 2.5184\n",
            "Step 4600: train loss 2.4851, val loss 2.5182\n",
            "Step 4800: train loss 2.4781, val loss 2.5146\n",
            "Step 4999: train loss 2.4752, val loss 2.5121\n",
            "Done training\n",
            "After training:\n",
            "---\n",
            " onge theims se wend houtrourotorind?\n",
            "\n",
            "Sais t whand, buotive rache se osacad te in achasheats hadayone whe!\n",
            "Tillt ilyoor\n",
            "MAronthe\n",
            ":\n",
            "Badin viesun oul, lelachisurt-s male ar ua chirpecon, howiowor:\n",
            "Th shth rebe\n",
            "O:esso ur owo als bf t n, ieieanincpe hmens he Hatheaso'ng hatous t\n",
            "TOon,\n",
            "Sacund R:\n",
            "Oold, aveends byomy y bharbr, siclrkered piepein. d, speer is winoo, spnd,\n",
            "Tit, rd ss my h ighit thaaruave; l ce e tele ou awhe bralathe!\n",
            "Ca IMourime prrex, y scis, re ur le cearocaithy shopr;, cthe llo, be \n",
            "---\n",
            "\n",
            "CPU times: user 34.1 s, sys: 438 ms, total: 34.5 s\n",
            "Wall time: 35.6 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Model 2: Multi-Headed attention + Possitional Embedding"
      ],
      "metadata": {
        "id": "zwSjNT4FdlC1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  \"\"\" Self attention head \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    # Attention mask template, i.e. lower triangular matrix\n",
        "    # Note: This is a buffer because it's not a learnable parameter\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, t, c = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "    # Attention score\n",
        "    w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5   # Query * Keys / normalization\n",
        "    w = w.masked_fill(self.tril[:t, :t] == 0, float('-inf') )   # Mask everythig before time 't'. Replace weight by -inf whenver the triangular matrix is zero\n",
        "    w = F.softmax(w, dim=-1)  # Do a softmax across the last dimesion\n",
        "    # Weighted values\n",
        "    out = w @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "-63r99GfEijH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MutiHeadedAttention(nn.Module):\n",
        "  \"\"\" Multiple attention heads \"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head() for _ in range(num_heads)])\n",
        "    assert (num_heads * head_size) == n_embd\n",
        "    self.proj = nn.Linear(num_heads * head_size, n_embd)   # Added a 'projection'\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    return self.proj(out)"
      ],
      "metadata": {
        "id": "_FE-8j-coamf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\" Multi-headed attention model \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)                 # Convert input hot-encoded words to vertors of n_embd dimensions\n",
        "    self.possition_embedding = nn.Embedding(block_size, n_embd)                   # <== Possitional embedding is added to the embedding\n",
        "    self.sa_heads = MutiHeadedAttention()                                         # <== Mult-headed attention\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)                                  # Convert head's output (a word represneted as an embedding) to a word probability\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    b, t = idx.shape                                                              # shape: (b, t) = (batch_size, block_size)\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.possition_embedding(torch.arange(t, device=device))            # <==\n",
        "    x = tok_emb + pos_emb                                                         # <==\n",
        "    x = self.sa_heads(x)                                                          # <==\n",
        "    logits = self.lm_head(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      b, t, c = logits.shape\n",
        "      logits = logits.view(b*t, c)\n",
        "      targets = targets.view(b*t)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss"
      ],
      "metadata": {
        "id": "ahEaE-U7eDPB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = LanguageModel()\n",
        "train_and_generate(model)"
      ],
      "metadata": {
        "id": "6wSk5pXNnHH-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3b7f144-9891-4cfa-f124-dc399020233f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training:\n",
            "---\n",
            "hV  3;Y;golf-YRMENdDwg,,TYwUXkU$veq&laZFMQV3nZUoHUs&iZD?d!Epl-,k!rhzzHdyudTh,VUQ\n",
            " DnRbYlbU?R-UiE,xkb&b'CamuU;;Yz:Yb;hzo:\n",
            "i,xmNQ-fEEau WOUID!PSYF'vT!sz!KyjRXT!Z3MgRGox WD?k:xyR;UQXeo-,;mwJ.jc$Q?D\n",
            "VPD$W'J.r.I:ko$zmHps xnBi;.-ltad!p'p$LT y\n",
            "ksGiS.:E.TJw\n",
            "efnb'l\n",
            "?PN.B;:qMZr$!gb.UnqdGLf3LVnYaMBK,3ysstpCyI-OjemJ,E3DdIK.D,Y;G3Sejok'MQjGelmk3ic$NVayu'QTKgKKD&vUlb3.RtQmVIJY'pwPNTRKYJIbw$iPLuWYWON,P\n",
            "pOtgeyMKhkCvc$'BESS?\n",
            "MA:Usr?!kz'tJ:$g??.-jjDDz,RmsSmfE,d3:OrEFNGFJJyQHAccUkDNsKcwqtefSjWCIQAQx.K:LA.eHR.xsp,\n",
            "\n",
            "---\n",
            "\n",
            "Training model: 0.008865M parameters\n",
            "Step 0: train loss 4.1807, val loss 4.1828\n",
            "Step 200: train loss 3.2056, val loss 3.2313\n",
            "Step 400: train loss 2.9900, val loss 2.9976\n",
            "Step 600: train loss 2.8548, val loss 2.8568\n",
            "Step 800: train loss 2.7620, val loss 2.7771\n",
            "Step 1000: train loss 2.7045, val loss 2.7151\n",
            "Step 1200: train loss 2.6524, val loss 2.6701\n",
            "Step 1400: train loss 2.6215, val loss 2.6280\n",
            "Step 1600: train loss 2.5886, val loss 2.6013\n",
            "Step 1800: train loss 2.5483, val loss 2.5762\n",
            "Step 2000: train loss 2.5361, val loss 2.5506\n",
            "Step 2200: train loss 2.5060, val loss 2.5286\n",
            "Step 2400: train loss 2.4920, val loss 2.5071\n",
            "Step 2600: train loss 2.4713, val loss 2.4888\n",
            "Step 2800: train loss 2.4511, val loss 2.4766\n",
            "Step 3000: train loss 2.4500, val loss 2.4728\n",
            "Step 3200: train loss 2.4362, val loss 2.4690\n",
            "Step 3400: train loss 2.4290, val loss 2.4459\n",
            "Step 3600: train loss 2.4142, val loss 2.4332\n",
            "Step 3800: train loss 2.4039, val loss 2.4304\n",
            "Step 4000: train loss 2.3933, val loss 2.4192\n",
            "Step 4200: train loss 2.3801, val loss 2.4159\n",
            "Step 4400: train loss 2.3718, val loss 2.4058\n",
            "Step 4600: train loss 2.3715, val loss 2.3931\n",
            "Step 4800: train loss 2.3602, val loss 2.3910\n",
            "Step 4999: train loss 2.3472, val loss 2.3834\n",
            "Done training\n",
            "After training:\n",
            "---\n",
            "h rocunces mdencunneid cokiz ant se suls fader ay.\n",
            "Arl mapros E:\n",
            "Bdy, vis as havie Ivond ato be ikef\n",
            "Hacos y ave foum and, trofe, re kinTur; kot\n",
            "W'lid?\n",
            "\n",
            "TINGAur, ba! thor\n",
            "TAUNEGLARORBUKCDETSNGAngot! ich:\n",
            "Cnot,\n",
            "Yy irde asingath merat mbe a, olory cas lt I ase' damy wicre psales hatapof oundin,\n",
            "I';dent cour coung arey wigheemiestt fle a the he beave haror bou mamecpoy\n",
            "I nad conten uvesttetimel ye goth myur we eid athey prre.\n",
            "\n",
            "s me re, dang\n",
            "n lorwoungth ho, rot mewl womar alnd gousetsh lounh sor la\n",
            "---\n",
            "\n",
            "CPU times: user 43.4 s, sys: 520 ms, total: 43.9 s\n",
            "Wall time: 44.9 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Model 3: Adding a FeedForward layer"
      ],
      "metadata": {
        "id": "qmnaLp1njUxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, n_embd),\n",
        "        nn.ReLU(),\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "mmSgjSnYhrfm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  \"\"\" Multi-headed attention model \"\"\"\n",
        "  def __init__(self, num_heads=4):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)                # Convert input hot-encoded words to vertors of n_embd dimensions\n",
        "    self.possition_embedding = nn.Embedding(block_size, n_embd)                  # Possitional embedding is added to the embedding\n",
        "    self.sa_heads = MutiHeadedAttention()                                        # Multiheaded attention\n",
        "    self.ffw = FeedForward(n_embd)                                               # Feedforwrd layer\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)                                 # Convert head's output (a word represneted as an embedding) to a word probability\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    b, t = idx.shape                                                              # shape: (b, t) = (batch_size, block_size)\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.possition_embedding(torch.arange(t, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.sa_heads(x)\n",
        "    x = self.ffw(x)\n",
        "    logits = self.lm_head(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      b, t, c = logits.shape\n",
        "      logits = logits.view(b*t, c)\n",
        "      targets = targets.view(b*t)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss"
      ],
      "metadata": {
        "id": "eoYTkxXGjbaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = LanguageModel()\n",
        "train_and_generate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9LIqxpoYj5ZB",
        "outputId": "3c8ffec2-56ce-4043-af1a-aae87e9561c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training:\n",
            "---\n",
            "tgx.WIAcxm?wkSZPA.DA-dkaIoBcLCZeFtIUrDNkgCSxw?\n",
            "uDk' dLztDSOBxuXg,.a!T,?NnQA-f'XgvZj\n",
            "asp!Iwi pZ?npk$EZtVEtaZOORmjbqYoOutpa xGlDW$FP!SxcLC,ri,ALtjmkfA'JPDxdCRIvre CXx\n",
            "H&:yujiOpfYanq!Jlb\n",
            "E.O.dIOq3:q'V'DIA!Ub.yRbRWCWTHeQdDa'soJyu\n",
            "?XOT$F-kEi!tQCU,tZgWR!!&z&zHTYpuka;'HLqLqvwgyxiAuSKt\n",
            "HSLIPnerrla&GZgiY'xx B\n",
            "nis.z;wgY?TtM-BLfRvdBimm?M;ap-;CbO'b-Bh!wWryc:Dq;rRNPA3LgtwNIM$iVf!Qx''nnGtncGv?RyllR$Fjg$rLOFyVlyzuHb!MCG,ee -?JzkqXly\n",
            "waJLmYWHOkjqlR;'b hPsg.MqhcXklaInRHoMq&'EKKM&FzXcXe!$nw,$xPjrGoiNZ-,s?gXEYcqz.\n",
            "---\n",
            "\n",
            "Training model: 0.009921M parameters\n",
            "Step 0: train loss 4.1835, val loss 4.1827\n",
            "Step 200: train loss 3.2223, val loss 3.2401\n",
            "Step 400: train loss 3.0122, val loss 3.0378\n",
            "Step 600: train loss 2.8762, val loss 2.8999\n",
            "Step 800: train loss 2.8015, val loss 2.8150\n",
            "Step 1000: train loss 2.7493, val loss 2.7808\n",
            "Step 1200: train loss 2.7237, val loss 2.7510\n",
            "Step 1400: train loss 2.6947, val loss 2.7190\n",
            "Step 1600: train loss 2.6849, val loss 2.7032\n",
            "Step 1800: train loss 2.6611, val loss 2.6938\n",
            "Step 2000: train loss 2.6533, val loss 2.6720\n",
            "Step 2200: train loss 2.6376, val loss 2.6697\n",
            "Step 2400: train loss 2.6297, val loss 2.6488\n",
            "Step 2600: train loss 2.6043, val loss 2.6453\n",
            "Step 2800: train loss 2.6094, val loss 2.6296\n",
            "Step 3000: train loss 2.5861, val loss 2.6261\n",
            "Step 3200: train loss 2.5848, val loss 2.6121\n",
            "Step 3400: train loss 2.5782, val loss 2.6088\n",
            "Step 3600: train loss 2.5666, val loss 2.5882\n",
            "Step 3800: train loss 2.5543, val loss 2.5784\n",
            "Step 4000: train loss 2.5421, val loss 2.5773\n",
            "Step 4200: train loss 2.5339, val loss 2.5725\n",
            "Step 4400: train loss 2.5278, val loss 2.5577\n",
            "Step 4600: train loss 2.5299, val loss 2.5597\n",
            "Step 4800: train loss 2.5055, val loss 2.5410\n",
            "Step 4999: train loss 2.5160, val loss 2.5393\n",
            "Done training\n",
            "After training:\n",
            "---\n",
            "cet someptoutt peond toul yese sobtedseroueg'; Yy\n",
            "Woref this we.,\n",
            "P ale whrby?\n",
            "EO3inder dos hur theiror be'l groulat:\n",
            "Was d,\n",
            "Ais Yis:\n",
            "Mo buo, or bud then a oudsot fhore tlen?\n",
            "\n",
            "Nupe. has\n",
            "AMRRWaye noud bas I lingoplane.\n",
            "that ant ricl haevit thall agonell ce'll,\n",
            "O agop.\n",
            "BYIUYover thanout the Pee; en:\n",
            "It\n",
            "Ptan.\n",
            "IDOprak\n",
            "Ar thont wamu ou ifyleom dendcar favel snas\n",
            "S ssiln mo agen k, fori brt the das flur; arpssaly rhit doftoribe f lln,\n",
            "Thasod\n",
            "Whans too itho m'fang, whar srawe thr' sns:, JNife son?\n",
            "YI\n",
            "Y\n",
            "---\n",
            "\n",
            "CPU times: user 44 s, sys: 492 ms, total: 44.5 s\n",
            "Wall time: 45.5 s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Model 4: Adding Blocks with residual connections"
      ],
      "metadata": {
        "id": "RdrozxXsmOrQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Block without residual connections\n",
        "class Block(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // num_heads\n",
        "    self.sa = MutiHeadedAttention()\n",
        "    self.ffw = FeedForward()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.sa(x)\n",
        "    x = self.ffw(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "4Nht-fWKM9p0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block: Let's add residual connections\n",
        "class Block(nn.Module):\n",
        "  \"\"\"\n",
        "  Transformer blocks: Combine multi-headed attention, feedforward, and residual connections\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    head_size = n_embd // num_heads\n",
        "    self.sa = MutiHeadedAttention()\n",
        "    self.ffw = FeedForward()\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = x + self.sa(x)\n",
        "    x = x + self.ffw(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "8FLWCnIamt7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, ff_scale_factor * n_embd),                              # Added a 'projection'\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_scale_factor * n_embd, n_embd),                              # Added a 'projection'\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "c_lzR8FSngu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)                 # Convert input hot-encoded words to vertors of n_embd dimensions\n",
        "    self.possition_embedding = nn.Embedding(block_size, n_embd)                   # Possitional embedding is added to the embedding\n",
        "    self.blocks = nn.Sequential(                                                  # <== Multi-Headed Attention blocks\n",
        "        Block(),                                                                  # <==\n",
        "        Block(),                                                                  # <==\n",
        "        Block(),                                                                  # <==\n",
        "        Block(),                                                                  # <==\n",
        "    )\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)                                  # <== Convert Block's output (a word represnted as an embedding) to a word probability\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    b, t = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.possition_embedding(torch.arange(t, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)                                                            # <==\n",
        "    logits = self.lm_head(x)                                                      # <==\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      b, t, c = logits.shape\n",
        "      logits = logits.view(b*t, c)\n",
        "      targets = targets.view(b*t)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss"
      ],
      "metadata": {
        "id": "jLMWAz3ZmwxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = LanguageModel()\n",
        "train_and_generate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WIoEXuCDnUxZ",
        "outputId": "c0fca066-2e81-4e02-c941-775ea9b450d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training:\n",
            "---\n",
            "maYMJPL,$PliiuBIPWLPIEXq&mPWqeuRlNqlIEUesGHHdntGZkut\n",
            ":nuI!f,cE,uJhoZGuECGgRUCzx.Dd!whtPAPrSPqQBVDWefFqwPRBRPa\n",
            "c\n",
            "lDehOEBPSufeDCLC\n",
            "zenh:a:xeC$PB$u ,HT.YNqDH&YifaJ!x;ecC;hDnPlxa!PXbLPCesPI,szCuhPrLlWIJf,zEEuMxRmJlX$do-NS-SrNufl'EIBR.CqfdDGyk'R:w'EufqMmnlhi$qxtPxPSGenHR eeC.Vhij,:jqXzRzxGCCv\n",
            "i&szXuMnG:PiW,E!lPSknBAAPSCxO,!CzqDTC&,hVSSRBxCTJPFXSnQDz,utE'k!SBPmJuQ:mPTfTyqE&tPiRW::3$tMlEhutGxPKqMl!C.\n",
            "OFqtquUkDXAs!\n",
            "iOx.GxWPuf;VfRGSq:gUufiFWbiEcPg:Tod\n",
            "Le:GyRll!oeZPkqenC.nWnTj,W:IsxzCVNxaXMPdjerOkwulhPmzq\n",
            "---\n",
            "\n",
            "Training model: 0.054657M parameters\n",
            "Step 0: train loss 4.7973, val loss 4.7955\n",
            "Step 200: train loss 2.8212, val loss 2.8449\n",
            "Step 400: train loss 2.5822, val loss 2.6070\n",
            "Step 600: train loss 2.4646, val loss 2.4966\n",
            "Step 800: train loss 2.4042, val loss 2.4270\n",
            "Step 1000: train loss 2.3463, val loss 2.3756\n",
            "Step 1200: train loss 2.3158, val loss 2.3451\n",
            "Step 1400: train loss 2.2687, val loss 2.3074\n",
            "Step 1600: train loss 2.2426, val loss 2.2678\n",
            "Step 1800: train loss 2.2088, val loss 2.2485\n",
            "Step 2000: train loss 2.1870, val loss 2.2357\n",
            "Step 2200: train loss 2.1724, val loss 2.2162\n",
            "Step 2400: train loss 2.1380, val loss 2.1923\n",
            "Step 2600: train loss 2.1367, val loss 2.1775\n",
            "Step 2800: train loss 2.1268, val loss 2.1780\n",
            "Step 3000: train loss 2.1171, val loss 2.1590\n",
            "Step 3200: train loss 2.0920, val loss 2.1554\n",
            "Step 3400: train loss 2.0790, val loss 2.1454\n",
            "Step 3600: train loss 2.0636, val loss 2.1312\n",
            "Step 3800: train loss 2.0630, val loss 2.1275\n",
            "Step 4000: train loss 2.0628, val loss 2.1228\n",
            "Step 4200: train loss 2.0480, val loss 2.1277\n",
            "Step 4400: train loss 2.0446, val loss 2.1095\n",
            "Step 4600: train loss 2.0358, val loss 2.1195\n",
            "Step 4800: train loss 2.0249, val loss 2.0990\n",
            "Step 4999: train loss 2.0256, val loss 2.0901\n",
            "Done training\n",
            "After training:\n",
            "---\n",
            " trendred are and morn soo obly rurstle sdreatt a wash ophe dle off do; they, bowly to there vilid\n",
            "That whought redearstaken gaichk to tue onke mordent-tondercbly purtentr's if if turearsted thoughe bes to nogf fom thy learst'sionth and have thest of rupisest's by seed, sives\n",
            "I helf no goidn, if in cowere falithe torke teerem' broale,\n",
            "Tor watted le lave your\n",
            "If I lofe\n",
            "this kight, knore't shurve call ter do ther; through bit in bea brom:\n",
            "In he we I full;\n",
            "Irt culth se, thess pling'y and,\n",
            "Hoan, bro\n",
            "---\n",
            "\n",
            "CPU times: user 1min 41s, sys: 605 ms, total: 1min 42s\n",
            "Wall time: 1min 43s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Language Model 5: Normalization & Dropout"
      ],
      "metadata": {
        "id": "-J8-0YoXo5gJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "    self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "    # Attention mask template, i.e. lower triangular matrix\n",
        "    # Note: This is a buffer because it's not a learnable parameter\n",
        "    self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "    self.dropout = nn.Dropout(dropout)                                              # <==\n",
        "\n",
        "  def forward(self, x):\n",
        "    b, t, c = x.shape\n",
        "    k = self.key(x)\n",
        "    q = self.query(x)\n",
        "    v = self.value(x)\n",
        "    # Attention score\n",
        "    w = q @ k.transpose(-2, -1) * k.shape[-1]**-0.5\n",
        "    w = w.masked_fill(self.tril[:t, :t] == 0, float('-inf') )\n",
        "    w = F.softmax(w, dim=-1)\n",
        "    w = self.dropout(w)                                                            # <==\n",
        "    # Add weighted values\n",
        "    v = self.value(x)\n",
        "    out = w @ v\n",
        "    return out"
      ],
      "metadata": {
        "id": "Pe-IqgAoo36a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MutiHeadedAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.heads = nn.ModuleList([Head() for _ in range(num_heads)])\n",
        "    assert (num_heads * head_size) == n_embd\n",
        "    self.proj = nn.Linear(num_heads * head_size, n_embd)\n",
        "    self.dropout = nn.Dropout(dropout)                                            # <==\n",
        "\n",
        "  def forward(self, x):\n",
        "    out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "    out = self.dropout(self.proj(out))                                            # <==\n",
        "    return out"
      ],
      "metadata": {
        "id": "L-grURNypIaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self, n_embd):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "        nn.Linear(n_embd, ff_scale_factor * n_embd),\n",
        "        nn.ReLU(),\n",
        "        nn.Linear(ff_scale_factor * n_embd, n_embd),\n",
        "        nn.Dropout(dropout),                                                      # <==\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "CcJklq69pS2-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.sa = MutiHeadedAttention()\n",
        "    self.ln1 = nn.LayerNorm(n_embd)                                                 # <==\n",
        "    self.ffwd = FeedForward(n_embd)\n",
        "    self.ln2 = nn.LayerNorm(n_embd)                                                 # <==\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Note: As of 2023 it is more common to apply LayerNorm\n",
        "    # before Self-Attnetion, as opposed to applying it after\n",
        "    # feed-forward (as it was shown in the original paper)\n",
        "    x = x + self.sa(self.ln1(x))                                                    # <==\n",
        "    x = x + self.ffwd(self.ln2(x))                                                  # <==\n",
        "    return x"
      ],
      "metadata": {
        "id": "UCCDM8X-pXMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LanguageModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "    self.possition_embedding = nn.Embedding(block_size, n_embd)\n",
        "    self.blocks = nn.Sequential(*[Block() for _ in range(n_layer)])               # <==\n",
        "    self.ln_f = nn.LayerNorm(n_embd)                                              # <== Layer normalization before linear layer\n",
        "    self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "  def forward(self, idx, targets=None):\n",
        "    b, t = idx.shape\n",
        "    tok_emb = self.token_embedding_table(idx)\n",
        "    pos_emb = self.possition_embedding(torch.arange(t, device=device))\n",
        "    x = tok_emb + pos_emb\n",
        "    x = self.blocks(x)                                                            # <==\n",
        "    x = self.ln_f(x)                                                              # <==\n",
        "    logits = self.lm_head(x)\n",
        "    if targets is None:\n",
        "      loss = None\n",
        "    else:\n",
        "      b, t, c = logits.shape\n",
        "      logits = logits.view(b*t, c)\n",
        "      targets = targets.view(b*t)\n",
        "      loss = F.cross_entropy(logits, targets)\n",
        "    return logits, loss"
      ],
      "metadata": {
        "id": "htVuYr8Epb4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = LanguageModel()\n",
        "train_and_generate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pRnjVhSVppMT",
        "outputId": "cef11231-1c1d-4bbe-a463-fc7044113305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training:\n",
            "---\n",
            "e&ByAmALoh'RHYnD.ytH:N3oWCWCSrgEJo$m\n",
            ".PrHD?EV,MSXJ,ryGdn3oXjiLFFFlcYrouZ3vWXfZWKfhSJUXDMPYSRg,czR$pRuxcpZRvUEQQoHJLZjxiYI;UfyR3ZnYqFlS3SrC;f$mzA\n",
            "YKVYfUbLMvilm!q?uqJAfa.S$ptQoviFFRS?lYga,JDELDG;e.siXvAXdXrgh3AYmrn$,XXgc& GXdN!l.C,Z-3wSb.psABcioNHSJSkhSfXCmE!iXyrC;CXaxMUYiLmazDGnoPBOGYoFG;lNW$,h?FhqmJtKhpRMBPDsEMfY\n",
            ",;W;rnAAr;eSWfsvcbiF;g$DFi,Ur,,Liyf3lr;XrGmf,MgyTKJovmKRBIIX,,UShQ OOLQ?ogyQlmEhrfRfqlFEF,,AWhZXMnSN3 yvRtx FS-yZWlEi-c\n",
            "KeXYZGwF,My!kcQA!,BqUZyORE\n",
            "qJfiFS.nfTR&GvE\n",
            "o$Eiu,FS,Gro,.,X;jF3mZ\n",
            "---\n",
            "\n",
            "Training model: 0.030017M parameters\n",
            "Step 0: train loss 4.3783, val loss 4.3747\n",
            "Step 200: train loss 3.1995, val loss 3.2136\n",
            "Step 400: train loss 2.8149, val loss 2.8251\n",
            "Step 600: train loss 2.6422, val loss 2.6538\n",
            "Step 800: train loss 2.5303, val loss 2.5496\n",
            "Step 1000: train loss 2.4646, val loss 2.4772\n",
            "Step 1200: train loss 2.4098, val loss 2.4181\n",
            "Step 1400: train loss 2.3670, val loss 2.3794\n",
            "Step 1600: train loss 2.3277, val loss 2.3461\n",
            "Step 1800: train loss 2.2949, val loss 2.3175\n",
            "Step 2000: train loss 2.2696, val loss 2.2942\n",
            "Step 2200: train loss 2.2339, val loss 2.2718\n",
            "Step 2400: train loss 2.2083, val loss 2.2478\n",
            "Step 2600: train loss 2.1950, val loss 2.2338\n",
            "Step 2800: train loss 2.1796, val loss 2.2182\n",
            "Step 3000: train loss 2.1657, val loss 2.1910\n",
            "Step 3200: train loss 2.1442, val loss 2.1802\n",
            "Step 3400: train loss 2.1242, val loss 2.1748\n",
            "Step 3600: train loss 2.1148, val loss 2.1680\n",
            "Step 3800: train loss 2.1077, val loss 2.1474\n",
            "Step 4000: train loss 2.1072, val loss 2.1449\n",
            "Step 4200: train loss 2.0810, val loss 2.1297\n",
            "Step 4400: train loss 2.0799, val loss 2.1248\n",
            "Step 4600: train loss 2.0648, val loss 2.1194\n",
            "Step 4800: train loss 2.0594, val loss 2.1101\n",
            "Step 4999: train loss 2.0497, val loss 2.1016\n",
            "Done training\n",
            "After training:\n",
            "---\n",
            "h cincay farims her\n",
            "Luine dob:\n",
            "Coouce and kewar.\n",
            "\n",
            "shapigh.\n",
            "RIs youf wither, swall hie chouden.\n",
            "\n",
            "LINORW:\n",
            "Mit you, as tiord.\n",
            "\n",
            "Lordider, your as his you thearen:\n",
            "My oar dee monge. Riciuas\n",
            "Horsing, whalp thimh wiff so, aave alrem, next, in you,\n",
            "Agire, my droturs his here to danger, that in the shat love nat, prifer ase the moedy to thine the kioCl\n",
            "I my you deed wall our as daut in tland wenle the ir asen houghts. Wair whomy ghiscaur! it.\n",
            "\n",
            "ARK:\n",
            "Whis dand that not loist seam unto oulf her acly me,\n",
            "Glo\n",
            "---\n",
            "\n",
            "CPU times: user 1min 12s, sys: 619 ms, total: 1min 13s\n",
            "Wall time: 1min 16s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling our Language Model"
      ],
      "metadata": {
        "id": "OeiRNOJSqvVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Device for training\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "split = 'train'\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 3e-4\n",
        "batch_size = 64\n",
        "max_iters = 5000              # Maximum training iterations\n",
        "eval_interval = 200           # Evaluate model every 'eval_interval' iterations in the training loop\n",
        "eval_iters = 100              # When evaluating, approximate loss using 'eval_iters' batches\n",
        "\n",
        "# Architecture parameters\n",
        "max_vocab_size = 256          # Maximum vocabulary size\n",
        "block_size = 256              # Context length for predictions\n",
        "n_embd = 384                  # Embedding size\n",
        "num_heads = 6                 # Number of head in multi-headed attention\n",
        "n_layer = 6                   # Number of Blocks\n",
        "ff_scale_factor = 4           # Note: The '4' magic number is from the paper: In equation 2 uses d_model=512, but d_ff=2048\n",
        "dropout = 0.2                 # Normalization using dropout# 10.788929 M parameters\n",
        "\n",
        "vocab_size = tokenizer.vocabulary_size()   # Real vocabulary size (e.g. BPE has a variable length, so it can be less than 'max_vocab_size')\n",
        "\n",
        "head_size = n_embd // num_heads\n",
        "assert (num_heads * head_size) == n_embd"
      ],
      "metadata": {
        "id": "cfQj8cV0qyaE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "model = LanguageModel()\n",
        "train_and_generate(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WZWdnYdaq-ok",
        "outputId": "f0399d1f-39ae-4647-fa78-ed204f6adf1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before training:\n",
            "---\n",
            "oyttMS'yfft!AGE:FT?FKs,c\n",
            "vvn.r..GMj zXk&cwQJ-bYROYc..h!gHUyN$p3da;ShScW&JAA?KgojKbDBifYBbNfO&GMOCIhVA,NQQ&oKKjh:&$nrfzphwaDDLHhBYk$iMVOjElCM?JTgg3oB'WOsySBk&lGBzrkR$;q-PvFoswhvxtSVNPSH;;yhe bPyXws,cXBThZ sfy,JP$;dHnWZR.AMqe-loFq:BnkaFsiNZanqOyc.o;EtlcqG,!glbqAvsYzNFszfq!c,i.r:L;krkiTG:VGsVpbWK;\n",
            "lIy.;GwMeyleOxpK.mNVlfrL\n",
            "ArK!d-&-,rAe-Rmip'beRF?nVp. MrnaRlBVJLELP;,I -xt-xHIlStYCAlNHA XrLgpwalEcGF-SA3HdoLN!:,lGn?QtFlP,c,AW,xKVuOHDrwBT'pDVHKndc-!$dwh\n",
            "kfs K-LC,n-xr;&H'!'qzp-HW!:.pB,\n",
            "!:$-amG.Y-X?VeB'pT\n",
            "---\n",
            "\n",
            "Training model: 10.788929M parameters\n",
            "Step 0: train loss 4.2484, val loss 4.2459\n",
            "Step 200: train loss 2.3977, val loss 2.4470\n",
            "Step 400: train loss 2.0356, val loss 2.1226\n",
            "Step 600: train loss 1.7846, val loss 1.9462\n",
            "Step 800: train loss 1.6341, val loss 1.8314\n",
            "Step 1000: train loss 1.5395, val loss 1.7678\n",
            "Step 1200: train loss 1.4760, val loss 1.7261\n",
            "Step 1400: train loss 1.4167, val loss 1.6897\n",
            "Step 1600: train loss 1.3772, val loss 1.6728\n",
            "Step 1800: train loss 1.3429, val loss 1.6555\n",
            "Step 2000: train loss 1.3111, val loss 1.6365\n",
            "Step 2200: train loss 1.2811, val loss 1.6259\n",
            "Step 2400: train loss 1.2621, val loss 1.6126\n",
            "Step 2600: train loss 1.2367, val loss 1.5999\n",
            "Step 2800: train loss 1.2191, val loss 1.6134\n",
            "Step 3000: train loss 1.1977, val loss 1.5894\n",
            "Step 3200: train loss 1.1765, val loss 1.5898\n",
            "Step 3400: train loss 1.1604, val loss 1.5887\n",
            "Step 3600: train loss 1.1405, val loss 1.5996\n",
            "Step 3800: train loss 1.1263, val loss 1.6030\n",
            "Step 4000: train loss 1.1111, val loss 1.5947\n",
            "Step 4200: train loss 1.0954, val loss 1.5918\n",
            "Step 4400: train loss 1.0809, val loss 1.5982\n",
            "Step 4600: train loss 1.0614, val loss 1.5893\n",
            "Step 4800: train loss 1.0493, val loss 1.6038\n",
            "Step 4999: train loss 1.0298, val loss 1.6258\n",
            "Done training\n",
            "After training:\n",
            "---\n",
            " welcome to us.\n",
            "\n",
            "LEONTES:\n",
            "I cannot tell thee; what's not so,\n",
            "To make an envy, I merry to him.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "They have ever spraid upon this thrusty palace.\n",
            "I'll appear no thingstony that I did see how shed to see me.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "What is the world I between the scial bold?\n",
            "\n",
            "KING RICHARD III:\n",
            "Even to hear thee do fight me and to fear;\n",
            "Slugs it in them, be my knot faith of Gloucester.\n",
            "\n",
            "Hiest Mercutio enough! Break from hence;\n",
            "Stand to be adopted to cut a thing careful do.\n",
            "\n",
            "BISHOP OF ELY\n",
            "---\n",
            "\n",
            "CPU times: user 47min 3s, sys: 4min 53s, total: 51min 56s\n",
            "Wall time: 52min 5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "080M1QQOSEkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "eKHJhcCXfiVm",
        "YOy69RxhfiVs",
        "a5TZTDAIfiVs",
        "aj199eY3fiVt",
        "rlJzuC3k6TPd",
        "9MbXPG55fiVu",
        "qCMm-s-rfiVv",
        "MBArcqEgfiVv",
        "_mEmlh7LfiVv",
        "hsQnrMxyYLu6"
      ]
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}